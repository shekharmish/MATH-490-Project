{"keys":[{"path":["title"],"id":"title","weight":1,"src":"title"},{"path":["body"],"id":"body","weight":1,"src":"body"}],"records":[{"i":0,"$":{"0":{"v":"This page has not yet sprouted","n":0.408},"1":{"v":"[Dendron](https://dendron.so/) (the tool used to generate this site) lets authors selective publish content. You will see this page whenever you click on a link to an unpublished page\n\n![](https://foundation-prod-assetspublic53c57cce-8cpvgjldwysl.s3-us-west-2.amazonaws.com/assets/images/not-sprouted.png)","n":0.189}}},{"i":1,"$":{"0":{"v":"Project","n":1},"1":{"v":"## MATH 490 Project\n- Author: Michael Volk\n- Date: 2022.12.13\n\n***\n## ยง Introduction\n![[1-introduction]]\n\n***\n## ยง GIN Theory\n![[2-theory]]\n\n***\n## ยง Experiments\n![[3-experiments]]\n\n***\n## ยง Future\n![[4-future]]\n\n","n":0.236}}},{"i":2,"$":{"0":{"v":"4 Future","n":0.707},"1":{"v":"- Compare GCN and GIN with the [WLConv](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.conv.WLConv), which equivalent $\\phi$ of the WL-kernel.\n- Run GCN and GIN over some small graphs where they are known to have different theoretical results. Showing these models can be trained and that they match theory will help build intuition around matching theory to experiment.\n","n":0.14}}},{"i":3,"$":{"0":{"v":"3 Experiments","n":0.707},"1":{"v":"### Models\nTo test the hypothesis that the GIN is more expressive we trained a GCN, GAT, and GIN model on a benchmark dataset provided by [OBG](https://ogb.stanford.edu/). The models are trained and then nodes representations are pooled to get a global representation of the graph for graph classification. All source code for the models can be found in the `pyg_gym` directory. Specifically the following files are used to train models:\n- [main.py](https://github.com/Mjvolk3/MATH-490-Project/blob/main/pyg_gym/main.py)\n- [metrics.py](https://github.com/Mjvolk3/MATH-490-Project/blob/main/pyg_gym/metrics.py)\n- [models.py](https://github.com/Mjvolk3/MATH-490-Project/blob/main/pyg_gym/models.py)\n- [mp.py](https://github.com/Mjvolk3/MATH-490-Project/blob/main/pyg_gym/mp.py)\n- [pool.py](https://github.com/Mjvolk3/MATH-490-Project/blob/main/pyg_gym/pool.py)\n- [runner.py](https://github.com/Mjvolk3/MATH-490-Project/blob/main/pyg_gym/runner.py)\n- [config_override.py](https://github.com/Mjvolk3/MATH-490-Project/blob/main/pyg_gym/config_override.py)\n\n### OGB Data\nTo familiarize yourself with the dataset we recommend reading the description of [obgb-ppa](https://ogb.stanford.edu/docs/graphprop/#ogbg-ppa). I have copied the relevant row from the data table provided. In brief, this is a graph classification tasks over 37 different classes.\n\n| Scale  | Name     | Package | Num Graphs | Num Nodes per graph | Num lEdges per graph | Num Tasks | Split Type | Task Type                  | Metric   |\n|--------|----------|---------|------------|---------------------|----------------------|-----------|------------|----------------------------|----------|\n| Medium | ogbg-ppa | >=1.1.1 | 158,100    | 243.4               | 2,266.1              | 1         | Species    | Multi-class classification | Accuracy |\n\n### Wandb\n\nAll experiments can be viewed on [wandb](https://wandb.ai/mjvolk3/MATH-490-Project?workspace=user-mjvolk3). This workspace should be public access so comment below if it is not.\n\n<!-- TODO update. -->\nAt the time of last commit, models were still in the process of training so I did not pull in their key results.","n":0.069}}},{"i":4,"$":{"0":{"v":"2 Theory","n":0.707},"1":{"v":"\n### Graph Kernels\n\nGraph kernels are used to compute the similarity between two graphs. $\\phi$ is used to map $G$ to a different space then similarity is computed in that space as the inner product $\\phi({G})^{\\top}\\phi({G^{\\prime}})$. Graph kernels are one of the traditional methods used for learning on graphs.\n\n### Weisfeiler-Lehman(WL) Kernel [^1]\n\nThe Weisfeiler-Lehman (WL) Kernel is a popular kernel for its time complexity and because of its expressiveness which is the ability to distinguish a large number of different types of graphs. WL Kernel computes computes $\\phi$ via the color refinement algorithm.\n\n[^1]: [Stanford CS224W Lecture 2.3](https://www.youtube.com/watch?v=buzsHTa4Hgs)\n\n> **Color Refinement Algorithm**:\n>\n> - Given a graph $G$ with a set of nodes $V$\n>     - Assign an initial color $c^{(0)}(v)$ to each node $v$\n>     - Iteratively refine node colors by\n>       - $C^{(k+1)}(v)=\\operatorname{HASH}\\left(\\left\\{c^{(k)}(v),\\left\\{c^{(k)}(u)\\right\\}_{u \\in N(v)}\\right\\}\\right)$\n>\n> - $\\operatorname{HASH}$ maps different inputs to different **colors**\n\nAfter $K$ steps of color refinement, $c^{K}(v)$ summarizes the structure of the $K$-hope neighborhood.\n\nThe $\\phi$ from the WL kernel counts the number of nodes of a given color. $\\phi$ represents a graph with a bag of colors. In more detail the kernel uses a generalized version of \"bag of node degrees\" since the node degrees are one-hop neighborhood information.\n\nThe WL kernel provides a strong benchmark for the development of GNNs since it is both computationally efficient and expressive. The cost to compute the WL kernel is linear in the number of edges since at each step information is aggregated at each node from its neighbors. Counting colors is linear w.r.t. the number of nodes, but since there are typically an order magnitude or more edges than nodes in real world graphs, the WL kernel time complexity is linear in the number of edges. Since we are discussing the kernel this is number of edges in both graphs.\n\n<!-- CHECK above \"but since there are typically an order magnitude or more edges than nodes in real world graphs\" -->\n\n### WL Graph Isomorphism Test\n\n<!-- CHECK -->\nThe WL Graph Isomorphism Test (WL test) only guarantees that two are graphs not isomorophic if $\\phi{(G)}^{\\top}\\phi{(G)} \\neq \\phi{(G)}^{\\top}\\phi{(G^{\\prime})}$, at any stage during color refinement. In general the WL test can give a measure of similarity or closeness to isomorphism between two graphs.\n\n\n### WL Test Toy Example\n\n![](./assets/drawio/WL-test.drawio.png)\n\nAs we can see in the example different colors capture different $k$-hop neighborhood structure. In this case $\\phi({G})^{\\top}\\phi({G^{\\prime}})=36$. We know from $\\phi({G})^{\\top}\\phi({G})$ and $\\phi({G^{\\prime}})^{\\top}\\phi({G^{\\prime}})$, that the two graphs are not isomorphic. In fact, we could have seen that these graphs were different after the first stage of color refinement.\n\n<!-- TODO fix this since the definition of the test is at any point discontinue -->\n### Graph Isomorphism Network\n\nIn How Powerful are Graph Neural Networks, the GIN is proven to be at least as expressive as the WL test, whereas the GCN is shown to worse than the WL test in some cases[^2].\n\n[^2]:[How Powerful are Graph Neural Networks?](https://arxiv.org/pdf/1810.00826.pdf)\n","n":0.046}}},{"i":5,"$":{"0":{"v":"1 Introduction","n":0.707},"1":{"v":"### [Message Passing Framework](https://pytorch-geometric.readthedocs.io/en/latest/notes/create_gnn.html)\nTraditionally machine learning over graphs has relied on graph statistics and kernel methods, the idea being to convert the graph in to a representation that can capture high level information about node neighbors and graph structure. These representations could then be used for a prediction task on the graph like a node or graph classification. Graph statistics and kernel methods are limited because they rely on hand engineered features and they don't adapt through the learning process. Additionally the handcrafted engineering of these features can be time consuming on reliant specialist domain knowledge. Instead the representation can be learned.\n\n![](/assets/images/1-introduction.md.node-embedding-learning-WL-Hamilton.png)\n\nThe idea is to learn a mapping from nodes $u$ and $v$ to $z_u$ and $z_v$ (Image[^1]). A decoder $DEC$ then decodes the embeddings vectors according to a supervised or unsupervised objective. A loss can then be back propagated through the $DEC$ and then the $ENC$ to update model weights. One way to do this is through the message passing framework.\n\nOne of the major motivations for the message passing framework is the fact that graph data is non-Euclidean, which precludes graphs from being input to a Euclidean convolutional neural network. The message passing framework generalized the convolution to non-Euclidean data. Furthermore it can be motivated by the color refinement algorithm that is presented in the Weisfeiler-Lehman(WL) graph isomorphism test.\n\n[^1]: [Graph Representation Learning Book](https://www.cs.mcgill.ca/~wlh/grl_book/)\n\nHere we present the message passing framework as presented by the popular [PyTorch Geometric Library](https://pytorch-geometric.readthedocs.io/en/latest/index.html). This definition of message passing will be used to define the different types of message passing neural networks.\n\n> $$\n> \\mathbf{x}_i^{(k)}=\\gamma^{(k)}\\left(\\mathbf{x}_i^{(k-1)}, \\square_{j \\in \\mathcal{N}(i)} \\phi^{(k)}\\left(\\mathbf{x}_i^{(k-1)}, \\mathbf{x}_j^{(k-1)}, \\mathbf{e}_{j, i}\\right)\\right)\n> $$\n>\n> $\\square_{j \\in \\mathcal{N}(i)}$ **:** Differentiable, permutation invariant function, *e.g.* sum, mean, or max.\n>\n> $\\phi^{(k)}$ **:** Differentiable functions such as Multi-Layer Perceptrons (MLPs).\n>\n> $\\gamma^{(k)}$ **:** Differentiable functions such MLPs.\n\n### Toy Example\n\nTo gain some intuition of a message passing framework we will take a small graph as example.\n\n![](./assets/drawio/Message-Passing-Framework.drawio.png)\n\nWe will consider messages passed to node $D$ in a 2-layer message passing neural network. The computational graph can be traced out from node $D$ first to a 1-hop neighborhood and then to a 2-hop neighborhood. The 1-hop neighborhood contains all nodes except node $A$. Including node $D$ is often considered optional and is can be interpreted as adding self-loops to the graph. Layer 0 looks shows connections from the 2-hop neighborhood of node $D$.\n\n### Popular Message Passing Neural Networks\n\n#### Graph Convolutional Neural Network ([GCN](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.conv.GCNConv))\n\n**Message Passing View**\n> $$\n> \\mathbf{x}_i^{\\prime}=\\boldsymbol{\\Theta}^{\\top} \\sum_{j \\in \\mathcal{N}(v) \\cup\\{i\\}} \\frac{e_{j, i}}{\\sqrt{\\hat{d}_j \\hat{d}_i}} \\mathbf{x}_j\n> $$\n>\n> $e_{i,j}$ **:** edge weight from node $j$ to node $i$ (i.e. with no edge weight, $e_{i,j} = \\mathbf{1}$).\n>\n> $\\hat{d}_{i} = 1 + \\sum_{j \\in \\mathcal{N}(i)}$ **:** weighted degree of node $i$, plus 1 to avoid divide by 0.\n>\n> $\\Theta^{\\top}$ **:** Weight matrix\n\n**Matrix View**\n> $X^l = \\sigma(\\hat{D}^{-1/2}\\hat{A}\\hat{D}^{-1/2}X^{(k-1)}\\boldsymbol{\\Theta})$\n>\n> $\\hat{A} = A + I$ **:** Adjacency matrix with inserted self loops.\n>\n> $\\hat{D}_{ij} = \\sum_{j=0}\\hat{A}_{ij}$ **:** Diagonal degree matrix.\n\nThe Graph Convolutional Neural (GCN) network was first published by Thomas Kipf and Max Welling at the international conference for learning representations (ICLR) in 2017 [^2]. This paper has had a tremendous impact on the development of graph neural networks and has sparked the design of more complicated and more expressive GNNs. To get an idea of the magnitude of impact, this paper currently has 14,095 citations whereas the famous transformer paper \"Attention is all you need\" has 59,941 citations.\n\n[^2]: [Semi-Supervised Classification with Graph Convolutional Networks](https://arxiv.org/abs/1609.02907)\n\n![](./assets/drawio/GCN.drawio.png)\n\n<!-- TODO update to OG Conv -->\n\nWe can look at the first layer of the GCN to see how it passes messages. Each node vector is multiplied by a weight matrix $\\boldsymbol{\\Theta}^{\\top}$ and then normalized by the product of degrees. This normalizing will prevent message vectors from exploding and it will remove degree bias.\n\n####  Graph Attention Network ([GAT](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.conv.GATConv))\n\n**Message Passing View**\n> $$\n> \\mathbf{x}_i^{\\prime}=\\alpha_{i, i} \\boldsymbol{\\Theta} \\mathbf{x}_i+\\sum_{j \\in \\mathcal{N}(i)} \\alpha_{i, j} \\boldsymbol{\\Theta} \\mathbf{x}_j\n> $$\n>\n> $$\n> \\alpha_{i, j}=\\frac{\\exp \\left(\\operatorname{LeakyReLU}\\left(\\mathbf{a}^{\\top}\\left[\\boldsymbol{\\Theta} \\mathbf{x}_i \\| \\boldsymbol{\\Theta} \\mathbf{x}_j\\right]\\right)\\right)}{\\sum_{k \\in \\mathcal{N}(i) \\cup\\{i\\}} \\exp \\left(\\operatorname{LeakyReLU}\\left(\\mathbf{a}^{\\top}\\left[\\boldsymbol{\\Theta} \\mathbf{x}_i \\| \\boldsymbol{\\Theta} \\mathbf{x}_k\\right]\\right)\\right)}\n> $$\n>\n> $||$ - Concatenation\n>\n> $\\Theta$ - Weight matrix. This is the same matrix in both $\\mathbf{x}^{\\prime}_i$ and $\\alpha_{i,j}$ equations.\n>\n> $\\mathbf{a}$ - Attention weights\n>\n> $\\alpha$ - Attention weight matrix\n\n**Matrix View**\n> $X^l = \\sigma(\\alpha AX^{(k-1)}\\boldsymbol{\\Theta})$\n\n<!-- CHECK is it theta transpose? check size. -->\n\nThe Graph Attention Network (GAT) was first published later in 2017 by Petar Veliฤkoviฤ. The GAT looks just like the GCN but it replaces the normalizing factor with an attention mechanism. Intuitively the attention weights will amplify important edges and suppress unimportant edges for the given prediction task[^3].\n\n![](./assets/drawio/GAT.drawio.png)\n\nThe attention mechanism was made famous in the transformer paper that used a pair wise attention across the entire input vector[^4]. Sometimes the attention mechanism applied to GAT is called masked attention, because it only considers edges within the underlying graph, \"masking\" or zeroing out all other attention coefficients[^3].\n\n[^3]: [Graph Attention Networks](https://arxiv.org/abs/1710.10903)\n\n[^4]: [Attention is All You Need](https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html)\n\n\nAttention weights are computed as a softmax of learned attention coefficients giving a value ranging from 0 to 1 for each attention weight in the attention matrix $\\alpha$. The sum over any given row or column will be 0. The attention matrix $\\alpha$ can be seen as weighted adjacency matrix where each of the non-zero elements is an attention weight. This maps on nicely to the matrix view of GAT.\n\nFor an in depth explanation see [Understanding Graph Attention Networks](https://www.youtube.com/watch?v=A-yKQamf2Fc) on YouTube or check out the original [Graph Attention Networks](https://arxiv.org/abs/1710.10903) Paper.\n\n<!-- - It was later revised in 2018 -->\n\n#### Graph Isomorphism Network ([GIN](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.conv.GINConv))\n\n**Message Passing View**\n> $$\n> \\mathbf{x}_i^{\\prime}=h_{\\Theta}\\left((1+\\epsilon) \\cdot \\mathbf{x}_i+\\sum_{j \\in \\mathcal{N}(i)} \\mathbf{x}_j\\right)\n> $$\n>\n> $\\epsilon$ **:** Hyperparameter that varies the impact of the self-loop.\n>\n> $h_{\\Theta}$ **:** A neural network, .i.e. an MLP.\n\n\n**Matrix View**\n>$X^l = h_{\\theta}(\\sigma(X^{(k-1)}))$\n\nThe Graph Isomorphism Network (GIN) is a more complex version of the GCN that was published in 2019 by Keyulu Xu et al[^5]. This idea can be rationalized by the universal approximation theory of neural networks that shows nearly any function can be approximated by a two layer neural network [^6]$^,$[^7]. By passing the node representations through multiple layers of a Multi-Layer Perceptron (MLP) the GIN is more complex in number of parameters but more expressive in in the data distributions that can be learned.\n\n[^5]:[How Powerful are Graph Neural Networks?](https://arxiv.org/pdf/1810.00826.pdf)\n[^6]:[Multilayer Feedforward Networks are Universal Approximators](https://www.sciencedirect.com/science/article/abs/pii/0893608089900208)\n[^7]:[Approximation Capabilities of Multilayer Feedforward Networks](https://www.sciencedirect.com/science/article/pii/089360809190009T)\n\n![](./assets/drawio/GIN.drawio.png)\n\n","n":0.031}}}]}
